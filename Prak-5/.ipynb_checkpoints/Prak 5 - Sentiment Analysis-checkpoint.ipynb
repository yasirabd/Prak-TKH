{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Praktikum 5 - Sentiment Analysis\n",
    "Sentiment Analysis merupakan salah satu bahasan dalam Natural Language Processing (NLP). Sentiment Analysis adalah proses untuk mendefinisikan sebuah teks termasuk ke dalam kategori <b>positif, negatif, </b>atau <b>neutral</b>. Hal ini juga dikenal sebagai opinion mining, berdasarkan opini atau sikap dari penulis/pembicara.\n",
    "\n",
    "## Agenda\n",
    "1. Simple Twitter Sentiment Analysis using NLTK\n",
    "2. Airline Twitter Sentiment Analysis (14640 tweets) and Predict New Tweet\n",
    "3. Sentiment Analysis Review Movies using Gensim Doc2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Simple Twitter Sentiment Analysis using NLTK\n",
    "Pada bagian ini, kita menggunakan 15 sample tweets yang 5 tweets berlabel positif, 5 tweets berlabel negatif, dan 5 tweets untuk test. Sentiment Analysis ini berdasarkan tutorial yang terdapat di [sini](http://www.laurentluce.com/posts/twitter-sentiment-analysis-using-python-and-nltk/). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pos_tweets = [('I love this car', 'positive'),\n",
    "              ('This view is amazing', 'positive'),\n",
    "              ('I feel great this morning', 'positive'),\n",
    "              ('I am so excited about the concert', 'positive'),\n",
    "              ('He is my best friend', 'positive')]\n",
    "\n",
    "neg_tweets = [('I do not like this car', 'negative'),\n",
    "              ('This view is horrible', 'negative'),\n",
    "              ('I feel tired this morning', 'negative'),\n",
    "              ('I am not looking forward to the concert', 'negative'),\n",
    "              ('He is my enemy', 'negative')]\n",
    "\n",
    "test = [('I feel happy this morning', 'positive'),\n",
    "        ('Larry is my friend', 'positive'),\n",
    "        ('I do not like that man', 'negative'),\n",
    "        ('My house is not great', 'negative'),\n",
    "        ('Your song is annoying', 'negative')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "- Dengan preprocessing sederhana, yaitu hanya menggunakan kata yang jumlah karakter lebih dari 3 akan termasuk data yang akan diproses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tweets = []\n",
    "for (words, sentiment) in pos_tweets + neg_tweets:\n",
    "    words_filtered = [e.lower() for e in words.split() if len(e) >= 3]\n",
    "    tweets.append((words_filtered, sentiment))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Hasil dari list tweets yang menggabungkan positif dan negatif tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Lakukan hal yang sama pada test tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_tweets = []\n",
    "for (words, sentiment) in test:\n",
    "    words_filtered = [e.lower() for e in words.split() if len(e) >= 3]\n",
    "    test_tweets.append((words_filtered, sentiment))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Hasil list dari test tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(test_tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifier\n",
    "Classifier yang digunakan adalah naive bayes.\n",
    "- List dari fitur kata pada variable <b>tweets</b> dilakukan ekstraksi. Sehingga diketahui jumlah frekuensi kemunculan kata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "def get_words_in_tweets(tweets):\n",
    "    all_words = []\n",
    "    for(words, sentiment) in tweets:\n",
    "        all_words.extend(words)\n",
    "    return all_words\n",
    "\n",
    "def get_word_features(wordlist):\n",
    "    wordlist = nltk.FreqDist(wordlist)\n",
    "    word_features = wordlist.keys()\n",
    "    return word_features\n",
    "\n",
    "word_features = get_word_features(get_words_in_tweets(tweets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(word_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Untuk membuat sebuah classifier, kita butuh untuk menentukan fitur apa saja yang relevan. Sehingga kita butuh feature extractor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_features(document):\n",
    "    document_words = set(document)\n",
    "    features = {}\n",
    "    for word in word_features:\n",
    "        features['contains(%s)' % word] = (word in document_words)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Sebagai contoh, terdapat document dengan kata <b>'love', 'this', 'car'</b> kemudian dilakukan extraksi fitur kata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "doc = ['love', 'this', 'car']\n",
    "\n",
    "print(extract_features(doc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Dengan adanya feature extractor, kita bisa menerapkan fitur pada classifier menggunakan fungsi <b>apply_features</b>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_set = nltk.classify.apply_features(extract_features, tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(training_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Setelah berhasil memiliki training set, sekarang kita dapat melakukan training pada classifier. Classifier yang digunakan adalah Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classifier = nltk.NaiveBayesClassifier.train(training_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive Bayes Classifier menggunakan prior probability pada setiap label yang merupakan frekuensi tiap label pada training set, dan kontribusi dari fitur."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classify\n",
    "Setelah kita memiliki classifier, kita bisa melakukan klasifikasi sebuah tweet dan melihat termasuk kategori apa tweet tersebut. Berikut adalah contohnya."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tweet = 'Larry is my friend'\n",
    "print(classifier.classify(extract_features(tweet.split())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tweet di atas termasuk kategori <b>positif</b> karena kata <b>friend</b> berasosiasi pada tweet positif <b>He is my best friend</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# mengeluarkan hasil positive karena feature name 'annoying' \n",
    "# belum terdapat pada data training\n",
    "tweet2 = 'Your song is annoying'\n",
    "print(classifier.classify(extract_features(tweet2.split())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sedangkan pada tweet di atas, kata <b>annoying</b> belum terdaftar sebagai salah satu fitur di training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Airline Twitter Sentiment Analysis (14640 tweets) and Predict New Tweet\n",
    "Pada bagian kedua, terinspirasi dari [kaggle](https://www.kaggle.com/) dalam melakukan sentiment analysis tweet airline. \n",
    "\n",
    "### Import Files dan Packages\n",
    "Sebagai persiapan download dataset <b>Tweets.csv</b> yang berisi tweet airline pada link [berikut](https://www.kaggle.com/crowdflower/twitter-airline-sentiment). Pastikan letakkan file pada lokasi yang berdekatan dengan file .ipynb. Pada praktikum ini, kami letakkan satu lokasi dengan file .ipynb "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import File and Packages\n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import re\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Read csv file into data frame\n",
    "tweet=pd.read_csv(\"Tweets.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and preprocess data using NLTK\n",
    "<b><i>Tokenize, Clean, Stem, Lemmatize, Remove stopwords</i></b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Preprocess the data {'negative': 0 , \n",
    "# 'positive': 1 , 'neutral': 2}\n",
    "\n",
    "df = tweet.iloc[:,(10,1)]\n",
    "df.columns = ['data', 'target']\n",
    "df['target'] = df['target'].str.strip().str.lower()\n",
    "df['target'] = df['target'].map({'negative': 0 , 'positive': 1 , 'neutral': 2})\n",
    "\n",
    "# Copy df to a temporary dataframe for pre-processing\n",
    "dft = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Remove @tweets, numbers, hyperlinks that do not start with letters\n",
    "dft['data'] = dft['data'].str.replace(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)|([0-9])\",\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# tokenize into words\n",
    "import nltk\n",
    "dft['data'] = dft['data'].apply(nltk.word_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# stem the tokens\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer = SnowballStemmer('english')\n",
    "dft['data'] = dft['data'].apply(lambda x: [stemmer.stem(y) for y in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Lemmatizing\n",
    "lemmatizer = nltk.WordNetLemmatizer()\n",
    "dft['data'] = dft['data'].apply(lambda x: [lemmatizer.lemmatize(y) for y in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Remove stopwords\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "# stem the stopwords\n",
    "stemmed_stops = [stemmer.stem(t) for t in stopwords]\n",
    "\n",
    "# remove stopwords from stemmed/lemmatized tokens\n",
    "dft['data'] = dft['data'].apply(lambda x: [stemmer.stem(y) for y in x if y not in stemmed_stops])\n",
    "\n",
    "# remove words whose length is <3\n",
    "dft['data'] = dft['data'].apply(lambda x: [e for e in x if len(e) >= 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Detokenize cleaned dataframe for vectorizing\n",
    "dft['data'] = dft['data'].str.join(\" \") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Printing data shape:\n",
    "mencetak shape dari dataset yang akan diproses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Print attributes of tweet, X and y\n",
    "print('Shape of original file : ', tweet.shape)\n",
    "print('All columns of the original file : ', tweet.columns.tolist() , '\\n')\n",
    "print('Columns dft dataframe : ',dft.columns.tolist(), '\\n') \n",
    "print('Shape data and target : ', dft['data'].shape, dft['target'].shape, '\\n')\n",
    "print('Mood Count target :\\n', tweet['airline_sentiment'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize X y :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "X = dft['data']\n",
    "y = dft['target']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print top features with freaquency:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_top_words():    \n",
    "    # Print top words\n",
    "    vect = CountVectorizer(stop_words='english',analyzer=\"word\", min_df = 2, max_df = 0.8)\n",
    "    data_dtm = vect.fit_transform(dft['data'])\n",
    "    feat_dtm = vect.get_feature_names()\n",
    "\n",
    "    # Count words\n",
    "    freq_tbl = pd.DataFrame({'Word':feat_dtm,'Occurence':np.asarray(data_dtm.sum(axis=0)).ravel().tolist()})\n",
    "    freq_tbl['Word'] = freq_tbl['Word'].str.strip()\n",
    "\n",
    "    # Print top words\n",
    "    topt = freq_tbl.sort(['Occurence'], ascending=[False]).head(10)\n",
    "    y = topt['Occurence']\n",
    "    plt.grid()\n",
    "    X = range(1, 11)\n",
    "    plt.bar(X,y,color='g')\n",
    "    plt.xlabel('Top words')\n",
    "    plt.ylabel('Occurence')\n",
    "    plt.title('Frequency of top 10 words')\n",
    "    plt.xticks(X,topt['Word'],rotation=90)\n",
    "    \n",
    "def print_top_neg_words():    \n",
    "    # Print top negative words\n",
    "    vect = CountVectorizer(stop_words='english',analyzer=\"word\", min_df = 2, max_df = 0.8)\n",
    "    filt = dft[dft['target'] == 0]\n",
    "    data_dtm = vect.fit_transform(filt['data'])\n",
    "    feat_dtm = vect.get_feature_names()\n",
    "\n",
    "    # Count words\n",
    "    freq_tbl = pd.DataFrame({'Word':feat_dtm,'Occurence':np.asarray(data_dtm.sum(axis=0)).ravel().tolist()})\n",
    "    freq_tbl['Word']=freq_tbl['Word'].str.strip()\n",
    "\n",
    "    # Print top negative words\n",
    "    topt = freq_tbl.sort(['Occurence'], ascending=[False]).head(10)\n",
    "    y = topt['Occurence']\n",
    "    plt.grid()\n",
    "    X = range(1, 11)\n",
    "    plt.bar(X,y,color='g')\n",
    "    plt.xlabel('Top negative words')\n",
    "    plt.ylabel('Occurence')\n",
    "    plt.title('Frequency of top 10 negative words')\n",
    "    plt.xticks(X,topt['Word'],rotation=90)\n",
    "    \n",
    "def print_top_pos_words():    \n",
    "    # Print top positive words\n",
    "    vect = CountVectorizer(stop_words='english',analyzer=\"word\", min_df = 2, max_df = 0.8)\n",
    "    filt = dft[dft['target'] == 1]\n",
    "    data_dtm = vect.fit_transform(filt['data'])\n",
    "    feat_dtm = vect.get_feature_names()\n",
    "\n",
    "    # Count words\n",
    "    freq_tbl = pd.DataFrame({'Word':feat_dtm,'Occurence':np.asarray(data_dtm.sum(axis=0)).ravel().tolist()})\n",
    "    freq_tbl['Word']=freq_tbl['Word'].str.strip()\n",
    "\n",
    "    # Print top positive words\n",
    "    topt = freq_tbl.sort(['Occurence'], ascending=[False]).head(10)\n",
    "    y = topt['Occurence']\n",
    "    plt.grid()\n",
    "    X = range(1, 11)\n",
    "    plt.bar(X,y,color='g')\n",
    "    plt.xlabel('Top positive words')\n",
    "    plt.ylabel('Occurence')\n",
    "    plt.title('Frequency of top 10 positive words')\n",
    "    plt.xticks(X,topt['Word'],rotation=90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Draw in matplotlib:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(1,figsize=(16, 16))\n",
    "plt.subplot(251)\n",
    "print_top_words()  \n",
    "plt.subplot(253)\n",
    "print_top_pos_words()\n",
    "plt.subplot(255)\n",
    "print_top_neg_words()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifying new tweets: Fit model, Clean Tweet, Predict Mood\n",
    "\n",
    "#### Fit Model : Logistic Regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "import time\n",
    "# Train Test split data with random state = 11\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.67, random_state=11)\n",
    "\n",
    "# Vectorize\n",
    "vect = CountVectorizer(stop_words='english',analyzer=\"word\",min_df = 2, max_df = 0.8)\n",
    "X_train_dtm = vect.fit_transform(X_train)\n",
    "X_test_dtm = vect.transform(X_test)\n",
    "feat_dtm = vect.get_feature_names()\n",
    "\n",
    "# Initialize classifier stats\n",
    "clf_stats = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Logistic Regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "clf = LogisticRegression()\n",
    "\n",
    "start_time = time.time()\n",
    "clf.fit(X_train_dtm, y_train)\n",
    "runtime = time.time()-start_time\n",
    "y_pred = clf.predict(X_test_dtm)\n",
    "accuracy = metrics.accuracy_score(y_test, y_pred)\n",
    "print('Accuracy : ',accuracy)\n",
    "\n",
    "# Store stats for classifier\n",
    "clf_stats = clf_stats.append({'Classifier': 'Logistic Regression',\n",
    "                              'Accuracy': accuracy, 'Runtime': runtime, 'Callable': 'clf = LogisticRegression()'}, \n",
    "                             ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clean incoming new tweet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Clean input tweet\n",
    "\n",
    "def fmt_input_tweet(txt):\n",
    "    \n",
    "    # Remove @tweets, numbers, hyperlinks that do not start with letters\n",
    "    txt = re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)|([0-9])\",\" \",txt)\n",
    "    #print(txt)\n",
    "    \n",
    "    # tokenize into words\n",
    "    tokens = [word for word in nltk.word_tokenize(txt)]\n",
    "    #print(tokens)\n",
    "\n",
    "    # only keep tokens that start with a letter (using regular expressions)\n",
    "    clean_tokens = [token for token in tokens if re.search(r'^[a-zA-Z]+', token)]\n",
    "    #print('clean_tokens:\\n',clean_tokens)\n",
    "\n",
    "    # stem the tokens\n",
    "    stemmer = SnowballStemmer('english')\n",
    "    stemmed_tokens = [stemmer.stem(t) for t in clean_tokens]\n",
    "    #print('stemmed_tokens:\\n',stemmed_tokens)\n",
    "\n",
    "    #Lemmatizing\n",
    "    lemmatizer = nltk.WordNetLemmatizer()\n",
    "    lem_tokens = [lemmatizer.lemmatize(t) for t in stemmed_tokens]\n",
    "    #print('lemmatizer : \\n',lem_tokens)\n",
    "    \n",
    "    #Remove stopwords\n",
    "    stopwords = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "    # stem the stopwords\n",
    "    stemmed_stops = [stemmer.stem(t) for t in stopwords]\n",
    "\n",
    "    # remove stopwords from stemmed/lemmatized tokens\n",
    "    lem_tokens_no_stop = [stemmer.stem(t) for t in lem_tokens if t not in stemmed_stops]\n",
    "\n",
    "    # remove words whose length is <3\n",
    "    clean_lem_tok = [e for e in lem_tokens_no_stop if len(e) >= 3]\n",
    "    #print('clean_lem_tok: ',clean_lem_tok)\n",
    "    \n",
    "    # Detokenize new tweet for vector processing\n",
    "    new_formatted_tweet=\" \".join(clean_lem_tok)\n",
    "    #print('new_formatted_tweet: ',new_formatted_tweet)\n",
    "    \n",
    "    return new_formatted_tweet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classify incoming new tweet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Vectorize, fit, transform. Select model randomly\n",
    "vect = CountVectorizer(stop_words='english', analyzer=\"word\", min_df = 2, max_df = 0.8)\n",
    "X_dtm = vect.fit_transform(X)\n",
    "feat_dtm = vect.get_feature_names()\n",
    "\n",
    "# Select the best performing classifier\n",
    "Call_clf = str(clf_stats[['Callable','Accuracy']].sort(['Accuracy'], ascending=[False]).head(1).iloc[:,(0)])\n",
    "temp = Call_clf.__repr__()\n",
    "Call_clf = temp[temp.index('c'):(temp.index(')'))+1]\n",
    "print('Model :',temp[(temp.index('=') + 1) : temp.index('(')])\n",
    "exec(Call_clf)\n",
    "clf.fit(X_dtm.toarray(), y) \n",
    "\n",
    "def classify_new_tweet(new_twt):  \n",
    "\n",
    "    fmt_twt = fmt_input_tweet(new_twt)\n",
    "    fmt_twt_dtm = vect.transform([fmt_twt])[0]\n",
    "    #print('Formatted Tweet :',fmt_twt)\n",
    "    pred = clf.predict(fmt_twt_dtm.toarray())\n",
    "\n",
    "    def mood(x):\n",
    "        return {\n",
    "            0: 'negative',\n",
    "            1: 'positive',\n",
    "            2: 'neutral'\n",
    "        }[x]\n",
    "\n",
    "    print('Mood of the incoming tweet is:',mood(pred[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict mood new tweet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# New input tweet\n",
    "twt='@united I am sick!! https://www.abc.com'\n",
    "classify_new_tweet(twt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Sentiment Analysis Review Movies using Gensim Doc2Vec\n",
    "Pada bagian ketiga, terinspirasi dari [word2vec-sentimen](http://linanqiu.github.io/2015/10/07/word2vec-sentiment/) dalam melakukan sentiment analysis pada review movie yang sangat mudah dilakukan.\n",
    "\n",
    "## Sentiment Analysis using Doc2Vec\n",
    "Menggunakan <b>Word2Vec</b> sangatlah mudah, secara singkat proses yang dilakukan yaitu terdapat input sebuah <b>corpus</b>, yang mengeluarkan hasil berupa <b>vectors dari setiap kata.</b>\n",
    "\n",
    "Dengan keluaran vectors ini, dapat membuat kata yang serupa akan saling berkaitan/berdekatan. Sebagai contoh kata <b>v_man - v_woman</b> sangat berkaitan dengan <b>v_king - v_queen</b>.\n",
    "Proses ini, dalam dunia NLP disebut dengan <b>word embedding</b> yaitu nama kolektif dalam language modelling dimana kata-kata atau frase dari kosakata dipetakan ke dalam vector.\n",
    "\n",
    "Dengan menggunakan Doc2Vec, kita bisa mewakilkan sebuah kalimat dengan sebuah vector dan dengan mudah kita dapat menjalankan algoritma klasifikasi sesuka kita. Bukankah menarik?    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "## Modules\n",
    "Disini menggunakan <b>gensim</b>, karena <b>gensim</b> memiliki banyak implementasi pada Word2Vec (dan Doc2Vec). Dan juga menggunakan <b>numpy</b> untuk manipulasi array dan <b>sklearn</b> untuk classifier Logistic Regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# gensim modules\n",
    "from gensim import utils\n",
    "from gensim.models.doc2vec import LabeledSentence\n",
    "from gensim.models import Doc2Vec\n",
    "\n",
    "# numpy\n",
    "import numpy\n",
    "\n",
    "# random\n",
    "from random import shuffle\n",
    "\n",
    "# classifier\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input Format\n",
    "Data input yang digunakan diambil dari Cornell IMDB movie review corpus (http://www.cs.cornell.edu/people/pabo/movie-review-data/). Namun data tersebut masih berupa raw.\n",
    "\n",
    "Oleh karena itu, dapat menggunakan data yang sudah dilakukan <b>clean</b> dengan dilakukan convert ke dalam lower case dan menghapus punctuation. Untuk data yang telah diolah dapat diperoleh di [sini](https://github.com/yasirabd/Prak-TKH/tree/master/Prak-5). Letakkan files pada satu lokasi dengan .ipynb.\n",
    "\n",
    "Berikut ini adalah document tersebut:\n",
    "- test-neg.txt: 12500 negative movie reviews from the test data\n",
    "- test-pos.txt: 12500 positive movie reviews from the test data\n",
    "- train-neg.txt: 12500 negative movie reviews from the training data\n",
    "- train-pos.txt: 12500 positive movie reviews from the training data\n",
    "- train-unsup.txt: 50000 Unlabelled movie reviews\n",
    "\n",
    "Sebagai catatan untuk mengecek document, <b>Setiap document terdapat dalam satu baris, dan dipisahkan dengan baris baru ke document berikutnya.</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feeding Data to Doc2Vec\n",
    "Fungsi Doc2Vec pada <b>gensim</b> memiliki kelemahan pada membaca data pada sebuah files. Hal ini dikarenakan pada class <b>LabeledLineSentence</b> yang berasal dari <b>LabeledSentence</b> (sebuah class dari <b>gensim.models.doc2vec</b>) hanya mewakilkan dari <b>satu kalimat</b>.\n",
    "\n",
    "Perbedaan dari Word2Vec dan Doc2Vec sendiri adalah\n",
    "\n",
    "Word2Vec melakukan convert sebuah kata menjadi vector.\n",
    "\n",
    "Doc2Vector melakukan convert seluruh kata pada kalimat menjadi sebuah vector. Oleh karena itu butuh <b>kata yang spesial</b> sebagai label pada kalimat tersebut.\n",
    "\n",
    "Sehingga format yang diinginkan adalah sebagai berikut:<br>\n",
    "<b>[['word1', 'word2', 'word3', 'lastword'], ['label1']]</b>\n",
    "\n",
    "<b>LabeledSentece</b> sudah sangat lebih rapi dalam membuat format seperti di atas yaitu menjadi - sebuah list dari kata dan label.\n",
    "\n",
    "Masalahnya pada kelas <b>LabeledLineSentence</b> hanya bisa melakukan \"perubahan\" pada satu files saja. Sedangkan kita butuh untuk multiple files, seperti misalnya pada multi documents (test, training, positive, negative, etc).\n",
    "\n",
    "Oleh karena itu kita akan membuat kelas <b>LabeledLineSentence</b> sendiri. Berikut adalah kelas yang telah dimodifikasi:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LabeledLineSentence(object):\n",
    "    def __init__(self, sources):\n",
    "        self.sources = sources\n",
    "        \n",
    "        flipped = {}\n",
    "        \n",
    "        # make sure that keys are unique\n",
    "        for key, value in sources.items():\n",
    "            if value not in flipped:\n",
    "                flipped[value] = [key]\n",
    "            else:\n",
    "                raise Exception('Non-unique prefix encountered')\n",
    "    \n",
    "    def __iter__(self):\n",
    "        for source, prefix in self.sources.items():\n",
    "            with utils.smart_open(source) as fin:\n",
    "                for item_no, line in enumerate(fin):\n",
    "                    yield LabeledSentence(utils.to_unicode(line).split(), [prefix + '_%s' % item_no])\n",
    "    \n",
    "    def to_array(self):\n",
    "        self.sentences = []\n",
    "        for source, prefix in self.sources.items():\n",
    "            with utils.smart_open(source) as fin:\n",
    "                for item_no, line in enumerate(fin):\n",
    "                    self.sentences.append(LabeledSentence(utils.to_unicode(line).split(), \n",
    "                                                          [prefix + '_%s' % item_no]))\n",
    "        return self.sentences\n",
    "    \n",
    "    def sentences_perm(self):\n",
    "        shuffle(self.sentences)\n",
    "        return self.sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dengan adanya kelas <b>LabeledLineSentence</b> yang baru, kita dapat melakukan input data file lebih dari satu. Kita perlu memasukkan <b>nama file</b> dan <b>special prefixes yang unik</b> pada tiap document.\n",
    "\n",
    "Berikut ini adalah penerapan pada praktikum:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sources = {'test-neg.txt':'TEST_NEG', \n",
    "           'test-pos.txt':'TEST_POS', \n",
    "           'train-neg.txt':'TRAIN_NEG', \n",
    "           'train-pos.txt':'TRAIN_POS', \n",
    "           'train-unsup.txt':'TRAIN_UNS'}\n",
    "\n",
    "sentences = LabeledLineSentence(sources)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "## Building the Vocabulary Table\n",
    "Doc2Vec membutuhkan sebuah <b>vocabulary table</b> yang akan memproses semua kata dan menyaring kata-kata yang unik serta melakukan perhitungan dasar. <b>model.build_vocab</b> butuh input sebuah array, oleh karena itu kita gunakan fungsi <b>to_array</b> dari kelas <b>LabeledLineSentences</b> yang telah didefinisikan di atas.\n",
    "\n",
    "Jika ingin mengetahui parameter pada Word2Vec documentation, berikut adalah penjelasannya:\n",
    "- <b>min_count</b>: ignore all words with total frequency lower than this. You have to set this to 1, since the sentence labels only appear once. Setting it any higher than 1 will miss out on the sentences.\n",
    "- <b>window</b>: the maximum distance between the current and predicted word within a sentence. Word2Vec uses a skip-gram model, and this is simply the window size of the skip-gram model.\n",
    "- <b>size</b>: dimensionality of the feature vectors in output. 100 is a good number. If youâ€™re extreme, you can go up to around 400.\n",
    "- <b>sample</b>: threshold for configuring which higher-frequency words are randomly downsampled\n",
    "- <b>workers</b>: use this many worker threads to train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "model = Doc2Vec(min_count=1, window=10, size=100, sample=1e-4, negative=5, \n",
    "                workers=8)\n",
    "\n",
    "model.build_vocab(sentences.to_array())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Doc2Vec\n",
    "Langkah selanjutnya adalah melakukan training pada model. <b>Model dilakukan training jika setiap epoch saat training, urutan dari kalimat yang dimasukkan dalam model dilakukan random.</b> Inilah alasan menggunakan fungsi <b>sentences_perm</b> pada kelas <b>LabeledLineSenteces</b>.\n",
    "\n",
    "Langkah ini termasuk yang paling lama. Pada Laptop Core i3 Ram 8GB membutuhkan waktu sekitar <b>1h 56min 39s.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "for epoch in range(10):\n",
    "    model.train(sentences.sentences_perm())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving and Loading Models\n",
    "Untuk mencegah melakukan training data yang butuh waktu lama, kita bisa menyimpannya dengan cara berikut."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "model.save('./imdb.d2v')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dan untuk melakukan load dengan cara berikut."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "model = Doc2Vec.load('./imdb.d2v')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspecting the Model\n",
    "Sekarang kita lihat model yang dihasilkan. Misalkan kata <b>good</b> kita dapat mencari kata yang paling mirip dengan <b>good</b>. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.most_similar('good')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kita juga bisa melihat vector hasil pembetukan model tersebut, misalkan kita ingin melihat contoh vector dari kalimat pertama pada training set untuk negative reviews:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.docvecs['TRAIN_NEG_0']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifying Sentiments\n",
    "## Training Vectors\n",
    "Sekarang kita gunakan vector tersebut untuk melakukan training pada classifier. Pertama, kita harus melakukan extract vector training. Disini kita memiliki 25000 training reviews, dimana positive 12500 dan negative 12500.\n",
    "\n",
    "Kemudian kita harus membentuk array dengan <b>numpy</b>, dimana akan dibentuk 2 array paralel, yang satu berisi vector <b>(train_arrays)</b> dan yang lainnya berisi label <b>(train_labels)</b>.\n",
    "\n",
    "Disini kita meletakkan positive reviews pertama kali, kemudian selanjutnya negative reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_arrays = numpy.zeros((25000, 100))\n",
    "train_labels = numpy.zeros(25000)\n",
    "\n",
    "for i in range(12500):\n",
    "    prefix_train_pos = 'TRAIN_POS_' + str(i)\n",
    "    prefix_train_neg = 'TRAIN_NEG_' + str(i)\n",
    "    train_arrays[i] = model.docvecs[prefix_train_pos]\n",
    "    train_arrays[12500 + i] = model.docvecs[prefix_train_neg]\n",
    "    train_labels[i] = 1\n",
    "    train_labels[12500 + i] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training Array akan tampak seperti berikut: dimana setiap baris mewakilkan setiap kalimat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(train_arrays)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sedangkan pada label vector yang dihasilkan yaitu, 1 mewakilkan positive sedangkan 0 mewakilkan negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Vectors\n",
    "Kita akan melakukan hal yang sama pada data testing. Hasil dari data testing dapat kita gunakan untuk melakukan evaluasi dari hasil."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_arrays = numpy.zeros((25000, 100))\n",
    "test_labels = numpy.zeros(25000)\n",
    "\n",
    "for i in range(12500):\n",
    "    prefix_test_pos = 'TEST_POS_' + str(i)\n",
    "    prefix_test_neg = 'TEST_NEG_' + str(i)\n",
    "    test_arrays[i] = model.docvecs[prefix_test_pos]\n",
    "    test_arrays[12500 + i] = model.docvecs[prefix_test_neg]\n",
    "    test_labels[i] = 1\n",
    "    test_labels[12500 + i] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification\n",
    "Selanjutnya kita lakukan training untuk <b>Logistic Regression</b> classifier menggunakan data training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "classifier = LogisticRegression()\n",
    "classifier.fit(train_arrays, train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Berikut adalah hasi accuracy yang didapatkan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "classifier.score(test_arrays, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
