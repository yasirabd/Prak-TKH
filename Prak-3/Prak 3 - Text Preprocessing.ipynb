{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Praktikum 3 - Text Preprocessing\n",
    "Text Preprocessing merupakan proses pengolahan dokumen untuk dibentuk menjadi bag of words atau dictionaries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "Tokenization merupakan proses pemisahan kalimat menjadi kata (token) atau urutan kata (n-grams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# text terdapat 3 teks\n",
    "# teks 1 berbahasa inggris\n",
    "# teks 2 berbahasa prancis\n",
    "# teks 3 berbahasa indonesia \n",
    "\n",
    "text_eng = \"She looked at   her father's arm-chair.\"\n",
    "text_fr = \"Qu'est-ce que c'est?\" \n",
    "text_id = \"Saya suka memakan lumpia asli Jawa Tengah. @LumpiaAsik\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# tokenisasi pada spaces\n",
    "print(text_eng.split(' '))\n",
    "print(text_fr.split(' '))\n",
    "print(text_id.split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# tokenisasi menggunakan scikit-learn\n",
    "# CountVectorizer menghapus kata yang hanya 1 karaker\n",
    "# CountVectorizer mengubah menjadi lowercase\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "print(CountVectorizer().build_tokenizer()(text_eng))\n",
    "print(CountVectorizer().build_tokenizer()(text_fr))\n",
    "print(CountVectorizer().build_tokenizer()(text_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# tokenisasi menggunakan nltk word_tokenizer\n",
    "# pada nltk word_tokenizer menggunakan TreebankWordTokenizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "print(word_tokenize(text_eng))\n",
    "print(word_tokenize(text_fr))\n",
    "print(word_tokenize(text_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# nltk TweetTokenizer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "tokenizer = TweetTokenizer(strip_handles=True, reduce_len=True)\n",
    "\n",
    "print(tokenizer.tokenize(text_eng))\n",
    "print(tokenizer.tokenize(text_fr))\n",
    "print(tokenizer.tokenize(text_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# menggunakan maketrans untuk tokenisasi spaces, dan tanda baca\n",
    "\n",
    "import string\n",
    "\n",
    "table = str.maketrans({ch: None for ch in string.punctuation})\n",
    "\n",
    "print([s.translate(table) for s in text_eng.split(' ') if s != ''])\n",
    "print([s.translate(table) for s in text_fr.split(' ') if s != ''])\n",
    "print([s.translate(table) for s in text_id.split(' ') if s != ''])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Stopwording\n",
    "Stopwording adalah proses penghilangan stopword yaitu kata-kata yang tidak memiliki makna jika berdiri sendiri."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# list kata-kata yang terdaftar pada stopwords\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "print(stopwords.words('english'))\n",
    "print(stopwords.words('indonesian'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# contoh stopword bahasa inggris\n",
    "stop_eng = set(stopwords.words('english'))\n",
    "\n",
    "sentence = \"This is a foo bar sentence\"\n",
    "\n",
    "print([i for i in sentence.lower().split() if i not in stop_eng])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# contoh stopword bahasa indonesia\n",
    "stop_id = set(stopwords.words('indonesian'))\n",
    "\n",
    "sentence = \"Ini adalah sebuah kalimat yang sangat aneh\"\n",
    "\n",
    "print([i for i in sentence.lower().split() if i not in stop_id])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming\n",
    "Stemming adalah proses mengubah kata-kata yang memiliki imbuhan menjadi kata dasar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Porter Stemmer\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "print(stemmer.stem('playing'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Sastrawi Stemmer\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "\n",
    "# create stemmer\n",
    "factory = StemmerFactory()\n",
    "stemmer = factory.create_stemmer()\n",
    "\n",
    "print(stemmer.stem('meniru-nirukannya'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
